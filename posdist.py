#python2
from nltk import word_tokenize
import math
import pickle
import matplotlib.pyplot as plt



""" Distribution of Part of Speech for Keyword in each Snapshot (week)
    For each week of texts, 
        [1] find all the sentences containing keyword and store in new list (to reduce time of POS tagging entire text!)
        [2] use NLTK POS Tagger to Tag all the text in the list of sentences
        [3] For keyword, count the number of each tag it has
        [4] Plot the distribution of POS for that word for each week
"""


# read in the list of texts generated by readCorpus
with open('texts.pkl', 'rb') as f:
   texts = pickle.load(f)

# Coresponding Dates for weeks 0 - 9
dates = ["Dec 03 - Dec 09", "Dec 10 - Dec 16", "Dec 17 - Dec 23", 
        "Dec 24 - Dec 30", "Dec 31 - Jan 06", "Jan 07 - Jan 13", 
        "Jan 14 - Jan 20", "Jan 21 - Jan 27", "Jan 28 - Feb 03", 
        "Feb 10 - Feb 17"]

# Now in texts[0] is all the text up to 00250023 and texts[1] has 00250023 to 00970095 and so on
# need to tokenize each element in texts and then calculate word frequencies, will use nltk
searchword = "union"
for i in range (0, len(texts)):
    raw = texts[i].decode('utf8').lower()

plt.plot([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dist, 'ro')
plt.show()

